- main framework ; knowledge distillation 작성  (ok)
- main framework ; node sparsity modeling 작성 (ok)

- experiment 부분
:★★★★★ knowledge distllation 과 sparsity modeling 간의 코드차이가 있어서 통일작업이 필요 : 2.23(목) 부 tuning 작업 완료!!! 
:★★★★★ sparsity modeling 을 0.5 pruning 했는데 0.8 pruning 하는 것 필요함 : 2.23(목) 해야함!!! 

 : 28x10 을 28x2로 0.8 pruning 하여서 실험결과 산출(pruning 작업) <-> knowledge distllation 을 28x10을 28x2로 작업
 : 28x2 을 28x1로 0.5 pruning 하여서 실험결과 산출(pruning 작업) <-> knowledge distllation 을 28x2을 28x1로 작업
 : dataset은 cifar 100과 cifar 10을 이요해서 실행

- 결론 : 군 관련 결론도 추가해보자(드론의 사물인식 과정을 위한 사물인식 시간, 정확도 / 모델의 크기를 압축하여 방대한 데이터가 오더라도 시간을 줄여서 pruning하는 작업의 효율성)


-------------------------------------------------------------------------------------------
chat GPT 조사 




<2. 24(금) 해야되는 사항>
1.wide-resnet 28x10, 28x2, 28x1 모델을 cifar10을 이용해서 돌리고 결과값 저장.  : 23.2.24 13:30 부 진행중
2. test결과 및 걸린시간, val loss accuracy 그래프로 저장

이 모든게 1번이 진행되야 기본 base 모델이 저장되는 것임

2.sparsity modeling : 28x2를 28x1로 0.5 pruning 작업과정 ok / 28x10을 28x2로 0.8 pruning 작업과정 / 28x4를 28x2로 0.5 pruning 작업과정(ok)
3.knowledge distillation : 28x10을 28x2로 distllation(ok) / 28x2를 28x1로 distillation(ok) / 28x4를 28x2로 distillation

Figure를 만드는 것까지 CIFAR10에서 모두 해보자
CIFAR100은 사실상 CLASSES 만 바뀌는 과정!! 

------------------------------------------------------

<2.28(화) 해야되는 사항>
1. CIFAR 100으로 전환해서 해보기(코드 작성)
2. CIFAR 10 / 100 결과를 표로 정리해보자
3. KD와 NODE SPARSITY MODLING 에서 비교해야 될 대상은 누구인지? ok
4. 표 결과 쓰기전에 result를 먼저써보쟈!! conclusion

- kd와 sparstiy modeling의 비교
- 장차 활용?? 



<3. 6(월)> 코드 정리
1. pruning code L1과 L2 결과 종합 (x) 
: node sparse만 해보자!! 
2. 종진이형 정리해준 fine_tuning code 정리해보기 ok 확인
 : 200epoch를 한번에 돌리는것이 아닌 20epoch씩 0.05비율로 pruning 하면 총 10번의 pruning 진행
3. 0.5 ** 1/10 을 10번 반복학습하면 0.5 pruning 한게 아닐까?
: ok 확인
4. weight sparse가 아닌 node sparse로 돌려야함!! 
: ok 확인
5. pruning 그림 준비!! 
6. 논문 쓸 그림 정리하자!! 
7. at one time / several times 비교

8 .메모리 사용량??
9. 파라미터 수 계산?? 
10. several pruning 에 대해서 epochs를 총 400을 생각해보자
11. one pruning의 결과값 기입, several pruning에 대해서 결과치를 끌어올려보자.... 어떻게 할지... 
- 군 기술에 얼마나 현실적으로 적용할 수 있을지 생각해보자. 


0.185 452
0.17 472
0.16 496
















